{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "import tweepy\n",
    "from transformers import T5Tokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\n",
    "    \"python ./transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
    "        --model_name_or_path=rinna/japanese-gpt2-small \\\n",
    "        --train_file=train_texts/timeline.txt \\\n",
    "        --do_train \\\n",
    "        --num_train_epochs=10 \\\n",
    "        --save_steps=10000 \\\n",
    "        --save_total_limit=3 \\\n",
    "        --per_device_train_batch_size=1 \\\n",
    "        --output_dir=finetuned_model/ \\\n",
    "        --overwrite_output_dir \\\n",
    "        --use_fast_tokenizer=False\",\n",
    "    shell=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
    "tokenizer.do_lower_case = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"../finetuned_model/\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_tweet(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: T5Tokenizer,\n",
    ") -> None:\n",
    "\n",
    "    client = tweepy.Client(\n",
    "        consumer_key=os.getenv(\"API_KEY\"),\n",
    "        consumer_secret=os.getenv(\"API_SECRET_KEY\"),\n",
    "        bearer_token=os.getenv(\"BEARER_TOKEN\"),\n",
    "        access_token=os.getenv(\"ACCESS_TOKEN\"),\n",
    "        access_token_secret=os.getenv(\"ACCESS_TOKEN_SECRET\"),\n",
    "    )\n",
    "\n",
    "    with open(\"../train_texts/timeline_today.txt\", \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    prompt = \"\"\n",
    "    while not prompt:\n",
    "        prompt = random.choice(data).replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"\\n\", \"\")\n",
    "\n",
    "    if len(prompt) > 10:\n",
    "        prompt = prompt[:10]\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=60,\n",
    "            min_length=10,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            bos_token_id=tokenizer.bos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            bad_word_ids=[[tokenizer.unk_token_id]],\n",
    "            repetition_penalty=0.99,\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "    decoded = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        \n",
    "    json = {}\n",
    "    json[\"text\"] = decoded[0]\n",
    "    \n",
    "    print(decoded[0])\n",
    "    return client._make_request(\"POST\", \"/2/tweets\", json=json, user_auth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_tweet(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tweetgen')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7825124f85d21765a3a1f5ea81747101f9f0cbd7d9a846ace745eea6d30ab384"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
